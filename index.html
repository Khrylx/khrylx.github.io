<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-KZEKLLQP31"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-KZEKLLQP31');
  </script>

  <link rel="stylesheet" href="assets/academicons/css/academicons.min.css">
  
  <title>Ye Yuan</title>
  
  <meta name="author" content="Ye Yuan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link href="data/misc/favicon.ico" rel="shortcut icon">
  <link href="data/misc/favicon_apple.ico" rel="apple-touch-icon">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="assets/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="assets/academicons/css/academicons.min.css">

  <script defer src="assets/js/fontawesome.all.min.js"></script>
  
  
</head>

<body>
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td class="intro">
              <p style="text-align:center">
                <name>Ye Yuan</name>
              </p>
              <p class="intro_paragraph">
                I'm a Research Scientist in the <a target="_blank" href="https://research.nvidia.com/labs/lpr/">Learning and Perception Team</a> at <a target="_blank" href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a>. I received my Ph.D. in <a target="_blank" href="http://ri.cmu.edu/">Robotics</a> from <a target="_blank" href="http://www.cmu.edu/">Carnegie Mellon University</a> in 2022, where I was advised by Prof. <a target="_blank" href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a>. I also earned my M.S. in computer science at CMU in 2016, where I worked with Prof. <a target="_blank" href="http://crl.ethz.ch/people/coros/">Stelian Coros</a>. I obtained my B.E. in computer science and technology from <a target="_blank" href="http://www.zju.edu.cn/english/">Zhejiang University</a> in 2015. My research has been supported by the <a target="_blank" href="https://www.qualcomm.com/invention/research/university-relations/innovation-fellowship/2020-north-america">Qualcomm Innovation Fellowship</a> and the <a target="_blank" href="https://www.nvidia.com/en-us/research/graduate-fellowships/graduate-fellowship-2021/">NVIDIA Graduate Fellowship</a>.
              </p>
              <p style="text-align:center">
                <a class="wrap" target="_blank" href="mailto:yyuan2@cs.cmu.edu"><i class="fa fa-envelope"></i> Email</a> &nbsp|&nbsp
                <a class="wrap" target="_blank" href="data/misc/CV.pdf"><i class="fa fa-file-pdf"></i> CV</a> &nbsp|&nbsp
                <a class="wrap" target="_blank" href="https://scholar.google.com/citations?user=EEp82sIAAAAJ&hl=en"><i class="ai ai-google-scholar ai-fw" style="font-size: 1.3em;position: relative; top:0.1em;margin-left: -0.3em;"></i>Google Scholar</a> &nbsp|&nbsp
                <a class="wrap" target="_blank" href="https://twitter.com/KhrylxYe"><i class="fab fa-twitter"></i> Twitter</a> &nbsp|&nbsp
                <a class="wrap" target="_blank" href="https://github.com/Khrylx"><i class="fab fa-github"></i> Github</a>
              </p>
            </td>
            <td class="headshort">
              <a href="data/misc/yeyuan.jpg" target="_blank"><img style="width:100%;max-width:100%" alt="profile photo" src="data/misc/yeyuan.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle;">
              <heading>News</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td class="news_date1">
              July
            </td>
            <td class="news_date2">
              2023
            </td>
            <td class="news_details">
              Two <a href="#gavatar">papers</a> accepted to <span class="news_conf">CVPR 2024</span>.
            </td>
          </tr>
          <tr>
            <td class="news_date1">
              July
            </td>
            <td class="news_date2">
              2023
            </td>
            <td class="news_details">
              Two <a href="#physdiff">papers</a> accepted to <span class="news_conf">ICCV 2023</span>. See you in Paris!
            </td>
          </tr>
          <tr>
            <td class="news_date1">
              Mar
            </td>
            <td class="news_date2">
              2023
            </td>
            <td class="news_details">
              One <a href="#vid2player3d">paper</a> on learning digital tennis player from videos accepted to <span class="news_conf">SIGGRAPH 2023 (Best Paper Honorable Mention)</span>.
            </td>
          </tr>
          <tr>
            <td class="news_date1">
              Feb
            </td>
            <td class="news_date2">
              2023
            </td>
            <td class="news_details">
              One <a href="#trace_pace">paper</a> on simulating pedestrian motions accepted to <span class="news_conf">CVPR 2023</span>.
            </td>
          </tr>
          <tr>
            <td class="news_date1">
              Sep
            </td>
            <td class="news_date2">
              2022
            </td>
            <td class="news_details">
              One <a href="#embody_scene">paper</a> on embodied human pose estimation accepted to <span class="news_conf">NeurIPS 2022</span>.
            </td>
          </tr>
          <tr>
            <td class="news_date1">
              May
            </td>
            <td class="news_date2">
              2022
            </td>
            <td class="news_details">
              Joined <a target="_blank" href="https://www.nvidia.com/en-us/research/">NVIDIA Research</a> as a Research Scientist.
            </td>
          </tr>
          <tr>
            <td class="news_date1">
              Apr
            </td>
            <td class="news_date2">
              2022
            </td>
            <td class="news_details">
              Defended my Ph.D. thesis <a target="_blank" href="https://arxiv.org/abs/2204.13678">Unified Simulation, Perception, and Generation of Human Behavior</a>.
            </td>
          </tr>
          <tr>
            <td class="news_date1">
              Mar
            </td>
            <td class="news_date2">
              2022
            </td>
            <td class="news_details">
              One <a href="#glamr">paper</a> on global human mesh recovery accepted to <span class="news_conf">CVPR 2022</span> with an <span class="news_oral">Oral Presentation</span>.
            </td>
          </tr>
          <tr>
            <td class="news_date1">
              Jan
            </td>
            <td class="news_date2">
              2022
            </td>
            <td class="news_details">
              One <a href="#transform2act">paper</a> on efficient automatic agent design accepted to <span class="news_conf">ICLR 2022</span> with an <span class="news_oral">Oral Presentation</span>.
            </td>
          </tr>
          <tr class="table_toggle" style="display:none;">
            <td class="news_date1">
              Jan
            </td>
            <td class="news_date2">
              2022
            </td>
            <td class="news_details">
              Invited Talk at <a href="https://ps.is.mpg.de/events/unified-simulation-perception-and-generation-of-human-behavior" target="_blank">MPI Perceiving Systems</a>.
            </td>
          </tr>
          <tr class="table_toggle" style="display:none;">
            <td class="news_date1">
              Sep
            </td>
            <td class="news_date2">
              2021
            </td>
            <td class="news_details">
              One <a href="#kin_poly">paper</a> on kinematics-guided control accepted to <span class="news_conf">NeurIPS 2021</span>.
            </td>
          </tr>
          <tr class="table_toggle" style="display:none;">
            <td class="news_date1">
              July
            </td>
            <td class="news_date2">
              2021
            </td>
            <td class="news_details">
              One <a href="#agentformer">paper</a> on multi-agent forecasting accepted to <span class="news_conf">ICCV 2021</span>.
            </td>
          </tr>

          <tr class="table_toggle" style="display:none;">
            <td class="news_date1">
              May
            </td>
            <td class="news_date2">
              2021
            </td>
            <td class="news_details">
              Starting my internship at NVIDIA AI.
            </td>
          </tr>

          <tr class="table_toggle" style="display:none;">
            <td class="news_date1">
              Apr
            </td>
            <td class="news_date2">
              2021
            </td>
            <td class="news_details">
              Invited Talk at ETH Zurich, Computer Vision and Learning Group.
            </td>
          </tr>

          <tr class="table_toggle" style="display:none;">
            <td class="news_date1">
              Apr
            </td>
            <td class="news_date2">
              2021
            </td>
            <td class="news_details">
              Invited Talk at "Machine Learning and Optimal Control" class, University of Alabama.
            </td>
          </tr>

          <tr class="table_toggle" style="display:none;">
            <td class="news_date1">
              Mar
            </td>
            <td class="news_date2">
              2021
            </td>
            <td class="news_details">
              Received an outstanding reviewer award by <span class="news_conf">ICLR 2021</span>.
            </td>
          </tr>

          <tr class="table_toggle" style="display:none;">
            <td class="news_date1">
              Feb
            </td>
            <td class="news_date2">
              2021
            </td>
            <td class="news_details">
              One <a href="#simpoe">paper</a> on physically-plausible human pose estimation accepted to <span class="news_conf">CVRP 2021</span> with an <span class="news_oral">Oral Presentation</span>.
            </td>
          </tr>

          <tr class="table_toggle" style="display:none;">
            <td class="news_date1">
              Feb
            </td>
            <td class="news_date2">
              2021
            </td>
            <td class="news_details">
              One <a href="#ptp">paper</a> accepted to <span class="news_conf">ICRA 2021</span>.
            </td>
          </tr>

          <tr class="table_toggle" style="display:none;">
            <td class="news_date1">
              Feb
            </td>
            <td class="news_date2">
              2021
            </td>
            <td class="news_details">
              Invited Talk at <a href="https://studentconference.csl.illinois.edu/program/technical-sessions/machine-learning-for-signal-processing/" target="_blank">16th CSL student conference</a>.
            </td>
          </tr>

          <tr class="table_toggle" style="display:none;">
            <td class="news_date1">
              Feb
            </td>
            <td class="news_date2">
              2021
            </td>
            <td class="news_details">
              Invited Talk at UIUC Robotics Seminar.
            </td>
          </tr>

          <tr class="table_toggle" style="display:none;">
            <td class="news_date1">
              Dec
            </td>
            <td class="news_date2">
              2020
            </td>
            <td class="news_details">
              Invited Talks at Qualcomm and Wayve.
            </td>
          </tr>

          <tr class="table_toggle" style="display:none;">
            <td class="news_date1">
              Dec
            </td>
            <td class="news_date2">
              2020
            </td>
            <td class="news_details">
              Honored to receive 2021 <a href="https://www.nvidia.com/en-us/research/graduate-fellowships/graduate-fellowship-2021/" target="_blank">NVIDIA Graduate Fellowship</a>.
            </td>
          </tr>

          <tr class="table_toggle" style="display:none;">
            <td class="news_date1">
              Sep
            </td>
            <td class="news_date2">
              2020
            </td>
            <td class="news_details">
              One paper on <a href="#rfc">Residual Force Control</a> accepted to <span class="news_conf">NeurIPS 2020</span>.
            </td>
          </tr>

          <tr class="table_toggle" style="display:none;">
            <td class="news_date1">
              Aug
            </td>
            <td class="news_date2">
              2020
            </td>
            <td class="news_details">
              Honored to receive 2020 <a href="https://www.qualcomm.com/invention/research/university-relations/innovation-fellowship/2020-north-america" target="_blank">Qualcomm Innovation Fellowship</a>.
            </td>
          </tr>

          <tr class="table_toggle" style="display:none;">
            <td class="news_date1">
              July
            </td>
            <td class="news_date2">
              2020
            </td>
            <td class="news_details">
              Two <a href="#dlow">papers</a> accepted to <span class="news_conf">ECCV 2020</span>.
            </td>
          </tr>

          <tr class="table_toggle" style="display:none;">
            <td class="news_date1">
              May
            </td>
            <td class="news_date2">
              2020
            </td>
            <td class="news_details">
              Starting my internship at Facebook Reality Lab Pittsburgh.</li>
            </td>
          </tr>

          <tr class="table_toggle" style="display:none;">
            <td class="news_date1">
              Feb
            </td>
            <td class="news_date2">
              2020
            </td>
            <td class="news_details">
              Two <a href="#nlospose">papers</a> accepted to <span class="news_conf">CVPR 2020</span>, <a href="#gen_forecast">one</a> with an <span class="news_oral">Oral Presentation</span>.
            </td>
          </tr>

          <tr class="table_toggle" style="display:none;">
            <td class="news_date1">
              Dec
            </td>
            <td class="news_date2">
              2019
            </td>
            <td class="news_details">
              Paper <a href="#dsf">Diverse Trajectory Forecasting with Determinantal Point Processes</a> accepted to <span class="news_conf">ICLR 2020</span>.
            </td>
          </tr>

          <tr class="table_toggle" style="display:none;">
            <td class="news_date1">
              July
            </td>
            <td class="news_date2">
              2019
            </td>
            <td class="news_details">
              Paper <a href="#ego_pose">Ego-Pose Estimation and Forecasting as Real-Time PD Control</a> accepted to <span class="news_conf">ICCV 2019</span>.
            </td>
          </tr>

          <tr class="table_toggle" style="display:none;">
            <td class="news_date1">
              July
            </td>
            <td class="news_date2">
              2018
            </td>
            <td class="news_details">
              Paper <a href="#gailpose">3D Ego-Pose Estimation via Imitation Learning</a> accepted to <span class="news_conf">ECCV 2018</span>.
            </td>
          </tr>          
        </tbody></table>

        <div style="margin-bottom:25px;padding: 3px 20px 0px 20px;">
          <a id="toggle_button" href="javascript:toggle()">Show more</a>
          <script>
            function toggle() {
              var rows = document.getElementsByClassName("table_toggle");
              var y = document.getElementById("toggle_button");
              if (rows[0].style.display == "none") {
                for (var i = 0; i < rows.length; i++) {
                  rows[i].style.display = "";
                }
                y.innerHTML = "Show less";
              } else {
                for (var i = 0; i < rows.length; i++) {
                  rows[i].style.display = "none";
                }
                y.innerHTML = "Show more";
              }
            }
          </script>
        </div>
        


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px 20px 0px 20px;width:100%;vertical-align:middle;">
              <heading>Research</heading>
              <p>
                My research lies at the intersection of computer vision, machine learning, and robotics. My current focus is on digital humans and generative AI for 3D. Example topics: 3D human avatar generation, human motion generation, in-the-wild human pose estimation, etc.
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr class="paper_entry" id="gavatar">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two"><video width=100% height=100% muted autoplay loop playsinline>
                <source src="data/thumbnails/gavatar_tn.mp4#t=0.001" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="https://nvlabs.github.io/GAvatar">
                <papertitle>GAvatar: Animatable 3D Gaussian Avatars with Implicit Mesh Learning</papertitle>
              </a>
              <br>
              <strong class="wrap">Ye Yuan</strong>*,
              <a class="wrap" target="_blank" href="https://sunshineatnoon.github.io/">Xueting Li</a>*,
              <a class="wrap" target="_blank" href="https://huangyangyi.github.io/">Yangyi Huang</a>,
              <a class="wrap" target="_blank" href="https://research.nvidia.com/person/shalini-de-mello">Shalini De Mello</a>,
              <a class="wrap" target="_blank" href="https://luminohope.org/">Koki Nagano</a>,
              <a class="wrap" target="_blank" href="https://jankautz.com/">Jan Kautz</a>,
              <a class="wrap" target="_blank" href="http://www.umariqbal.info/">Umar Iqbal</a> &nbsp; (*Equal Contribution)
              <br>
              <span class="paper_conf">CVPR</span>, <span class="paper_year">2024</span>
              <br>
              <a class="wrap" target="_blank" href="https://nvlabs.github.io/GAvatar"><i class="fa fa-globe"></i> project page</a> | 
              <a class="wrap" target="_blank" href="https://arxiv.org/abs/2312.11461"><i class="fa fa-file-pdf"></i> arXiv</a> | 
              <a class="wrap" target="_blank" href="https://youtu.be/PbCF1HzrKrs"><i class="fab fa-youtube"></i> video</a>
            </td>
          </tr>

          <tr class="paper_entry" id="physdiff">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two"><video width=100% height=100% muted autoplay loop playsinline>
                <source src="data/thumbnails/physdiff_tn.mp4#t=0.001" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="https://nvlabs.github.io/PhysDiff">
                <papertitle>PhysDiff: Physics-Guided Human Motion Diffusion Model</papertitle>
              </a>
              <br>
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="https://tsong.me/">Jiaming Song</a>, 
              <a class="wrap" target="_blank" href="http://www.umariqbal.info/">Umar Iqbal</a>, 
              <a class="wrap" target="_blank" href="http://latentspace.cc/">Arash Vahdat</a>,
              <a class="wrap" target="_blank" href="https://jankautz.com/">Jan Kautz</a>
              <br>
              <span class="paper_conf">ICCV</span>, <span class="paper_year">2023</span> &nbsp <strong class="wrap paper_oral">(Oral Presentation)</strong>
              <br>
              <a class="wrap" target="_blank" href="https://nvlabs.github.io/PhysDiff"><i class="fa fa-globe"></i> project page</a> | 
              <a class="wrap" target="_blank" href="https://arxiv.org/abs/2212.02500"><i class="fa fa-file-pdf"></i> arXiv</a> | 
              <a class="wrap" target="_blank" href="https://youtu.be/y8Tdcvzjfjg"><i class="fab fa-youtube"></i> video</a>
            </td>
          </tr>

          <tr class="paper_entry" id="av_pose">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two"><video width=100% height=100% muted autoplay loop playsinline>
                <source src="data/thumbnails/av_pose_tn.mp4#t=0.001" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="">
                <papertitle>Learning Human Dynamics in Autonomous Driving Scenarios</papertitle>
              </a>
              <br>
              <a class="wrap" target="_blank" href="https://scholar.google.com.hk/citations?user=GStTsxAAAAAJ&hl=en">Jingbo Wang</a>, 
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="https://zhengyiluo.github.io/">Zhengyi Luo</a>, 
              <a class="wrap" target="_blank" href="https://kevincxie.github.io/">Kevin Xie</a>, 
              <a class="wrap" target="_blank" href="http://dahua.site/">Dahua Lin</a>, 
              <a class="wrap" target="_blank" href="http://www.umariqbal.info/">Umar Iqbal</a>, 
              <a class="wrap" target="_blank" href="https://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a>, 
              <a class="wrap" target="_blank" href="https://www.samehkhamis.com/">Sameh Khamis</a>
              <br>
              <span class="paper_conf">ICCV</span>, <span class="paper_year">2023</span>
              <br>
              <a class="wrap" target="_blank" href=""><i class="fa fa-file-pdf"></i> arXiv (coming soon)</a>
            </td>
          </tr>

          <tr class="paper_entry" id="vid2player3d">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two"><video width=100% height=100% muted autoplay loop playsinline>
                <source src="data/thumbnails/vid2player3d_tn.mp4#t=0.001" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="https://research.nvidia.com/labs/toronto-ai/vid2player3d/">
                <papertitle>Learning Physically Simulated Tennis Players from Broadcast Videos</papertitle>
              </a>
              <br>
              <a class="wrap" target="_blank" href="https://cs.stanford.edu/~haotianz/">Haotian Zhang</a>, 
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="https://www.linkedin.com/in/makoviychuk-viktor-9199988/">Viktor Makoviychuk</a>, 
              <a class="wrap" target="_blank" href="https://www.linkedin.com/in/kelly-guo-18316982/?originalSubdomain=ca">Yunrong Guo</a>, 
              <a class="wrap" target="_blank" href="https://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a>, 
              <a class="wrap" target="_blank" href="https://xbpeng.github.io/">Xue Bin Peng</a>, 
              <a class="wrap" target="_blank" href="http://graphics.stanford.edu/~kayvonf/">Kayvon Fatahalian</a>
              <br>
              <span class="paper_conf">SIGGRAPH</span>, <span class="paper_year">2023</span> &nbsp <strong class="wrap paper_oral">(Best Paper Honorable Mention)</strong>
              <br>
              <a class="wrap" target="_blank" href="https://research.nvidia.com/labs/toronto-ai/vid2player3d/"><i class="fa fa-globe"></i> project page</a> | 
              <a class="wrap" target="_blank" href="https://research.nvidia.com/labs/toronto-ai/vid2player3d/data/tennis_skills_main.pdf"><i class="fa fa-file-pdf"></i> paper</a> | 
              <a class="wrap" target="_blank" href="https://youtu.be/ZZVKrNs7_mk"><i class="fab fa-youtube"></i> video</a>
            </td>
          </tr>

          <tr class="paper_entry" id="trace_pace">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two"><video width=100% height=100% muted autoplay loop playsinline>
                <source src="data/thumbnails/trace_pace_tn.mp4#t=0.001" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="https://nv-tlabs.github.io/trace-pace/">
                <papertitle>Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion</papertitle>
              </a>
              <br>
              <a class="wrap" target="_blank" href="https://davrempe.github.io/">Davis Rempe</a>, 
              <a class="wrap" target="_blank" href="https://zhengyiluo.github.io/">Zhengyi Luo</a>, 
              <a class="wrap" target="_blank" href="https://xbpeng.github.io/">Xue Bin Peng</a>, 
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a>, 
              <a class="wrap" target="_blank" href="https://www.cs.utoronto.ca/~fidler/">Sanja Fidler</a>, 
              <a class="wrap" target="_blank" href="https://orlitany.github.io/">Or Litany</a>
              <br>
              <span class="paper_conf">CVPR</span>, <span class="paper_year">2023</span>
              <br>
              <a class="wrap" target="_blank" href="https://nv-tlabs.github.io/trace-pace/"><i class="fa fa-globe"></i> project page</a> | 
              <a class="wrap" target="_blank" href="https://arxiv.org/abs/2304.01893"><i class="fa fa-file-pdf"></i> arXiv</a> | 
              <a class="wrap" target="_blank" href="https://nv-tlabs.github.io/trace-pace/supp.html"><i class="fab fa-youtube"></i> demo</a>
            </td>
          </tr>

          <tr class="paper_entry" id="rgb_nerf">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two">
                  <img src="data/thumbnails/rgb_nerf_tn.png">
                </div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="https://ngp-mpc.github.io/">
                <papertitle>RGB-Only Reconstruction of Tabletop Scenes for Collision-Free Manipulator Control</papertitle>
              </a>
              <br>
              <a class="wrap" target="_blank" href="https://recordmp3.github.io/">Zhenggang Tang</a>, 
              <a class="wrap" target="_blank" href="https://balakumar-s.github.io/">Balakumar Sundaralingam</a>, 
              <a class="wrap" target="_blank" href="https://research.nvidia.com/person/jonathan-tremblay">Jonathan Tremblay</a>,
              <a class="wrap" target="_blank" href="https://wenbowen123.github.io/">Bowen Wen</a>,
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="https://research.nvidia.com/person/stephen-tyree">Stephen Tyree</a>, 
              <a class="wrap" target="_blank" href="https://research.nvidia.com/person/charles-loop">Charles Loop</a>, 
              <a class="wrap" target="_blank" href="https://www.alexander-schwing.de/">Alexander Schwing</a>, 
              <a class="wrap" target="_blank" href="https://cecas.clemson.edu/~stb/">Stan Birchfield</a>
              <br>
              <span class="paper_conf">ICRA</span>, <span class="paper_year">2023</span>
              <br>
              <a class="wrap" target="_blank" href="https://ngp-mpc.github.io/"><i class="fa fa-globe"></i> project page</a> | 
              <a class="wrap" target="_blank" href="https://arxiv.org/abs/2210.11668"><i class="fa fa-file-pdf"></i> arXiv</a> | 
              <a class="wrap" target="_blank" href="https://youtu.be/Bn1w9y5aYYg"><i class="fab fa-youtube"></i> video</a> | 
              <a class="wrap" target="_blank" href="https://drive.google.com/drive/folders/15Ch7pJM8znw8l4-wHu6rJvDdwRoNqI8l?usp=share_link"><i class="fa fa-file-archive"></i> data</a>
            </td>
          </tr>

          <tr class="paper_entry" id="embody_scene">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two"><video width=100% height=100% muted autoplay loop playsinline>
                <source src="data/thumbnails/embody_scene.mp4#t=0.001" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="https://embodiedscene.github.io/embodiedpose/">
                <papertitle>Embodied Scene-aware Human Pose Estimation</papertitle>
              </a>
              <br>
              <a class="wrap" target="_blank" href="https://zhengyiluo.github.io/">Zhengyi Luo</a>, 
              <a class="wrap" target="_blank" href="https://sh8.io/">Shun Iwase</a>, 
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a>
              <br>
              <span class="paper_conf">NeurIPS</span>, <span class="paper_year">2022</span>
              <br>
              <a class="wrap" target="_blank" href="https://embodiedscene.github.io/embodiedpose/"><i class="fa fa-globe"></i> project page</a> | 
              <a class="wrap" target="_blank" href="https://arxiv.org/abs/2206.09106"><i class="fa fa-file-pdf"></i> arXiv</a> | 
              <a class="wrap" target="_blank" href="https://youtu.be/8Ae0xzqAtm8"><i class="fab fa-youtube"></i> video</a>
            </td>
          </tr>
          
          <tr class="paper_entry" id="phd_thesis">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two">
                  <img src="data/thumbnails/phd_thesis_tn.png">
                </div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="https://arxiv.org/abs/2204.13678">
                <papertitle>Unified Simulation, Perception, and Generation of Human Behavior</papertitle>
              </a>
              <br>
              <strong class="wrap">Ye Yuan</strong>
              <br>
              <span class="paper_conf">Ph.D. Thesis, Robotics Institute, CMU,</span> <span class="paper_year">2022</span>
              <br>
              <a class="wrap" target="_blank" href="https://arxiv.org/abs/2204.13678"><i class="fa fa-file-pdf"></i> arXiv</a>
            </td>
          </tr>
          
          <tr class="paper_entry" id="glamr">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two"><video width=100% height=100% muted autoplay loop playsinline>
                <source src="data/thumbnails/glamr_tn.mp4#t=0.001" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="https://nvlabs.github.io/GLAMR">
                <papertitle>GLAMR: Global Occlusion-Aware Human Mesh Recovery with Dynamic Cameras</papertitle>
              </a>
              <br>
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="http://www.umariqbal.info/">Umar Iqbal</a>, 
              <a class="wrap" target="_blank" href="https://research.nvidia.com/person/pavlo-molchanov/">Pavlo Molchanov</a>, 
              <a class="wrap" target="_blank" href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a>,
              <a class="wrap" target="_blank" href="https://jankautz.com/">Jan Kautz</a>
              <br>
              <span class="paper_conf">CVPR</span>, <span class="paper_year">2022</span> &nbsp <strong class="wrap paper_oral">(Oral Presentation - Top 4.2%)</strong>
              <br>
              <a class="wrap" target="_blank" href="https://nvlabs.github.io/GLAMR"><i class="fa fa-globe"></i> project page</a> | 
              <a class="wrap" target="_blank" href="https://arxiv.org/abs/2112.01524"><i class="fa fa-file-pdf"></i> arXiv</a> | 
              <a class="wrap" target="_blank" href="https://youtu.be/wpObDXcYueo"><i class="fab fa-youtube"></i> video</a> | 
              <a class="wrap" target="_blank" href="https://github.com/NVlabs/GLAMR"><i class="fab fa-github"></i> code</a>
            </td>
          </tr>

          <tr class="paper_entry" id="transform2act">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two"><video width=100% height=100% muted autoplay loop playsinline>
                <source src="data/thumbnails/transform2act_tn.mp4#t=0.001" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="https://sites.google.com/view/transform2act">
                <papertitle>Transform2Act: Learning a Transform-and-Control Policy for Efficient Agent Design</papertitle>
              </a>
              <br>
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="https://yudasong.github.io/">Yuda Song</a>, 
              <a class="wrap" target="_blank" href="https://zhengyiluo.github.io/">Zhengyi Luo</a>, 
              <a class="wrap" target="_blank" href="https://wensun.github.io/">Wen Sun</a>, 
              <a class="wrap" target="_blank" href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a>
              <br>
              <span class="paper_conf">ICLR</span>, <span class="paper_year">2022</span> &nbsp <strong class="wrap paper_oral">(Oral Presentation - Top 1.6%)</strong>
              <br>
              <a class="wrap" target="_blank" href="https://sites.google.com/view/transform2act"><i class="fa fa-globe"></i> project page</a> | 
              <a class="wrap" target="_blank" href="https://arxiv.org/abs/2110.03659"><i class="fa fa-file-pdf"></i> arXiv</a> | 
              <a class="wrap" target="_blank" href="https://openreview.net/forum?id=UcDUxjPYWSr"><i class="fa fa-file-pdf"></i> openreview</a> | 
              <a class="wrap" target="_blank" href="https://github.com/Khrylx/Transform2Act"><i class="fab fa-github"></i> code</a>
            </td>
          </tr>

          <tr class="paper_entry" id="online_meta">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two">
                  <img src="data/thumbnails/online_meta_tn.png" style="position: relative;clip: rect(50px,50px,0px,0px);">
                </div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="https://arxiv.org/abs/2204.01925">
                <papertitle>Online No-regret Model-Based Meta RL for Personalized Navigation</papertitle>
              </a>
              <br>
              <a class="wrap" target="_blank" href="https://yudasong.github.io/">Yuda Song</a>, 
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="https://wensun.github.io/">Wen Sun</a>, 
              <a class="wrap" target="_blank" href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a>
              <br>
              <span class="paper_conf">Learning for Dynamics & Control (L4DC)</span>, <span class="paper_year">2022</span>
              <br>
              <a class="wrap" target="_blank" href="https://arxiv.org/abs/2204.01925"><i class="fa fa-file-pdf"></i> paper</a>
            </td>
          </tr>

          <tr class="paper_entry" id="kin_poly">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two">
                  <img src="data/thumbnails/kinpoly_tn.gif">
                </div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="https://zhengyiluo.github.io/projects/kin_poly/">
                <papertitle>Dynamics-Regulated Kinematic Policy for Egocentric Pose Estimation</papertitle>
              </a>
              <br>
              <a class="wrap" target="_blank" href="https://zhengyiluo.github.io/">Zhengyi Luo</a>, 
              <a class="wrap" target="_blank" href="https://ryohachiuma.github.io/">Ryo Hachiuma</a>, 
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a>
              <br>
              <span class="paper_conf">NeurIPS</span>, <span class="paper_year">2021</span>
              <br>
              <a class="wrap" target="_blank" href="https://zhengyiluo.github.io/projects/kin_poly/"><i class="fa fa-globe"></i> project page</a> | 
              <a class="wrap" target="_blank" href="https://arxiv.org/abs/2106.05969"><i class="fa fa-file-pdf"></i> arXiv</a> | 
              <a class="wrap" target="_blank" href="https://youtu.be/yEiK9K1N-zw"><i class="fab fa-youtube"></i> video</a> | 
              <a class="wrap" target="_blank" href="https://github.com/KlabCMU/kin-poly"><i class="fab fa-github"></i> code</a>
            </td>
          </tr>

          <tr class="paper_entry" id="agentformer">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two">
                  <img src="data/thumbnails/agentformer_tn.png">
                </div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="https://www.ye-yuan.com/agentformer">
                <papertitle>AgentFormer: Agent-Aware Transformers for Socio-Temporal Multi-Agent Forecasting</papertitle>
              </a>
              <br>
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="https://www.xinshuoweng.com/">Xinshuo Weng</a>, 
              <a class="wrap" target="_blank" href="https://scholar.google.com/citations?user=9W0Bd00AAAAJ&hl=en">Yanglan Ou</a>, 
              <a class="wrap" target="_blank" href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a>
              <br>
              <span class="paper_conf">ICCV</span>, <span class="paper_year">2021</span>
              <br>
              <a class="wrap" target="_blank" href="https://www.ye-yuan.com/agentformer"><i class="fa fa-globe"></i> project page</a> | 
              <a class="wrap" target="_blank" href="https://arxiv.org/abs/2103.14023"><i class="fa fa-file-pdf"></i> arXiv</a> | 
              <a class="wrap" target="_blank" href="https://github.com/Khrylx/AgentFormer"><i class="fab fa-github"></i> code</a>
            </td>
          </tr>

          <tr class="paper_entry" id="simpoe">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two">
                  <img src="data/thumbnails/simpoe_tn.png">
                </div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="https://www.ye-yuan.com/simpoe">
                <papertitle>SimPoE: Simulated Character Control for 3D Human Pose Estimation</papertitle>
              </a>
              <br>
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="https://scholar.google.com/citations?user=sFQD3k4AAAAJ&hl=en&oi=sra">Shih-En Wei</a>, 
              <a class="wrap" target="_blank" href="http://www.cs.cmu.edu/~tsimon/">Tomas Simon</a>, 
              <a class="wrap" target="_blank" href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a>, 
              <a class="wrap" target="_blank" href="https://scholar.google.com/citations?user=ss-IvjMAAAAJ&hl=en">Jason Saragih</a>
              <br>
              <span class="paper_conf">CVPR</span>, <span class="paper_year">2021</span> &nbsp <strong class="wrap paper_oral">(Oral Presentation - Top 4.2%)</strong>
              <br>
              <a class="wrap" target="_blank" href="https://www.ye-yuan.com/simpoe"><i class="fa fa-globe"></i> project page</a> | 
              <a class="wrap" target="_blank" href="https://arxiv.org/abs/2104.00683"><i class="fa fa-file-pdf"></i> arXiv</a> | 
              <a class="wrap" target="_blank" href="https://youtu.be/icdsZfVBT80"><i class="fab fa-youtube"></i> talk</a> | 
              <a class="wrap" target="_blank" href="https://youtu.be/_0BtkjOrwX4"><i class="fab fa-youtube"></i> video</a>
            </td>
          </tr>

          <tr class="paper_entry" id="ptp">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two">
                  <img src="data/thumbnails/ptp_tn.png">
                </div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="https://www.xinshuoweng.com/projects/PTP/">
                <papertitle>PTP: Parallelized 3D Tracking and Prediction with Graph Neural Networks and Diversity Sampling</papertitle>
              </a>
              <br>
              <a class="wrap" target="_blank" href="https://www.xinshuoweng.com/">Xinshuo Weng</a>*, 
              <strong class="wrap">Ye Yuan</strong>*, 
              <a class="wrap" target="_blank" href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a> &nbsp; (*Equal Contribution)
              <br>
              <span class="paper_conf">RA-L and ICRA</span>, <span class="paper_year">2021</span> &nbsp <strong class="wrap paper_oral">(Best Student Paper Candidate < 2%)</strong>
              <br>
              <a class="wrap" target="_blank" href="https://www.xinshuoweng.com/projects/PTP/"><i class="fa fa-globe"></i> project page</a> | 
              <a class="wrap" target="_blank" href="https://arxiv.org/abs/2003.07847"><i class="fa fa-file-pdf"></i> arXiv</a> | 
              <a class="wrap" target="_blank" href="https://github.com/xinshuoweng/PTP"><i class="fab fa-github"></i> code</a>
            </td>
          </tr>

          <tr class="paper_entry" id="rfc">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two"><video width=100% height=100% muted autoplay loop playsinline>
                <source src="data/thumbnails/rfc_tn.mp4#t=0.001" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="https://www.ye-yuan.com/rfc">
                <papertitle>Residual Force Control for Agile Human Behavior Imitation and Extended Motion Synthesis</papertitle>
              </a>
              <br>
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a>
              <br>
              <span class="paper_conf">NeurIPS</span>, <span class="paper_year">2020</span>
              <br>
              <a class="wrap" target="_blank" href="https://www.ye-yuan.com/rfc"><i class="fa fa-globe"></i> project page</a> | 
              <a class="wrap" target="_blank" href="https://arxiv.org/abs/2006.07364"><i class="fa fa-file-pdf"></i> arXiv</a> | 
              <a class="wrap" target="_blank" href="https://youtu.be/XuzH1u78o1Y"><i class="fab fa-youtube"></i> video</a> | 
              <a class="wrap" target="_blank" href="https://github.com/Khrylx/RFC"><i class="fab fa-github"></i> code</a>
            </td>
          </tr>

          <tr class="paper_entry" id="dlow">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two">
                  <img src="data/thumbnails/dlow_tn.png">
                </div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="https://www.ye-yuan.com/dlow">
                <papertitle>DLow: Diversifying Latent Flows for Diverse Human Motion Prediction</papertitle>
              </a>
              <br>
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a>
              <br>
              <span class="paper_conf">ECCV</span>, <span class="paper_year">2020</span>
              <br>
              <a class="wrap" target="_blank" href="https://www.ye-yuan.com/dlow"><i class="fa fa-globe"></i> project page</a> | 
              <a class="wrap" target="_blank" href="https://arxiv.org/abs/2003.08386"><i class="fa fa-file-pdf"></i> arXiv</a> | 
              <a class="wrap" target="_blank" href="https://youtu.be/c45ss6Tcb2A"><i class="fab fa-youtube"></i> talk</a> | 
              <a class="wrap" target="_blank" href="https://youtu.be/nVYGHnRB1_M"><i class="fab fa-youtube"></i> summary</a> | 
              <a class="wrap" target="_blank" href="https://youtu.be/64OEdSadb00"><i class="fab fa-youtube"></i> video</a> | 
              <a class="wrap" target="_blank" href="https://github.com/Khrylx/DLow"><i class="fab fa-github"></i> code</a>
            </td>
          </tr>

          <tr class="paper_entry" id="c2nlos">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two">
                  <img src="data/thumbnails/c2nlos_tn.png">
                </div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="https://marikoisogawa.github.io/project/c2nlos">
                <papertitle>Efficient Non-Line-of-Sight Imaging from Transient Sinograms</papertitle>
              </a>
              <br>
              <a class="wrap" target="_blank" href="https://marikoisogawa.github.io/">Mariko Isogawa</a>, 
              <a class="wrap" target="_blank" href="https://dorianchan.com/">Dorian Yao Chan</a>, 
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a>, 
              <a class="wrap" target="_blank" href="http://www.cs.cmu.edu/~motoole2/">Matthew O'Toole</a>
              <br>
              <span class="paper_conf">ECCV</span>, <span class="paper_year">2020</span>
              <br>
              <a class="wrap" target="_blank" href="https://www.ye-yuan.com/dlow"><i class="fa fa-globe"></i> project page</a> | 
              <a class="wrap" target="_blank" href="https://arxiv.org/abs/2008.02787"><i class="fa fa-file-pdf"></i> arXiv</a> | 
              <a class="wrap" target="_blank" href="https://youtu.be/iGhstZG2SCg"><i class="fab fa-youtube"></i> summary</a> | 
              <a class="wrap" target="_blank" href="https://youtu.be/Ec3lLi4SbX8"><i class="fab fa-youtube"></i> video</a>
            </td>
          </tr>

          <tr class="paper_entry" id="dsf">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two"><video width=100% height=100% muted autoplay loop playsinline>
                <source src="data/thumbnails/dsf_tn.mp4#t=0.001" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="https://openreview.net/forum?id=ryxnY3NYPS">
                <papertitle>Diverse Trajectory Forecasting with Determinantal Point Processes</papertitle>
              </a>
              <br>
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a>
              <br>
              <span class="paper_conf">ICLR</span>, <span class="paper_year">2020</span>
              <br>
              <a class="wrap" target="_blank" href="https://arxiv.org/abs/1907.04967"><i class="fa fa-file-pdf"></i> arXiv</a> | 
              <a class="wrap" target="_blank" href="https://openreview.net/forum?id=ryxnY3NYPS"><i class="fa fa-file-pdf"></i> openreview</a> | 
              <a class="wrap" target="_blank" href="https://youtu.be/5i71SU_IdS4"><i class="fab fa-youtube"></i> video</a>
            </td>
          </tr>

          <tr class="paper_entry" id="nlospose">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two">
                  <img src="data/thumbnails/nlospose_tn.png">
                </div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="https://marikoisogawa.github.io/project/nlos_pose">
                <papertitle>Optical Non-Line-of-Sight Physics-based 3D Human Pose Estimation</papertitle>
              </a>
              <br>
              <a class="wrap" target="_blank" href="https://marikoisogawa.github.io/">Mariko Isogawa</a>, 
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="http://www.cs.cmu.edu/~motoole2/">Matthew O'Toole</a>, 
              <a class="wrap" target="_blank" href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a>
              <br>
              <span class="paper_conf">CVPR</span>, <span class="paper_year">2020</span>
              <br>
              <a class="wrap" target="_blank" href="https://marikoisogawa.github.io/project/nlos_pose"><i class="fa fa-globe"></i> project page</a> | 
              <a class="wrap" target="_blank" href="https://arxiv.org/abs/2003.14414"><i class="fa fa-file-pdf"></i> arXiv</a> | 
              <a class="wrap" target="_blank" href="https://youtu.be/4HFulrdmLE8"><i class="fab fa-youtube"></i> video</a> | 
              <a class="wrap" target="_blank" href="https://github.com/marikoisogawa/OpticalNLOSPose"><i class="fab fa-github"></i> code</a>
            </td>
          </tr>

          <tr class="paper_entry" id="gen_forecast">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two">
                  <img src="data/thumbnails/gen_forecast_tn.png">
                </div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="https://arxiv.org/abs/1904.06250">
                <papertitle>Generative Hybrid Representations for Activity Forecasting with No-Regret Learning</papertitle>
              </a>
              <br>
              <a class="wrap" target="_blank" href="http://jiaqi.web.illinois.edu">Jiaqi Guan</a>, 
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a>, 
              <a class="wrap" target="_blank" href="https://people.eecs.berkeley.edu/~nrhinehart/">Nick Rhinehart</a>
              <br>
              <span class="paper_conf">CVPR</span>, <span class="paper_year">2020</span> &nbsp <strong class="wrap paper_oral">(Oral Presentation - Top 5.7%)</strong>
              <br>
              <a class="wrap" target="_blank" href="https://arxiv.org/abs/1904.06250"><i class="fa fa-file-pdf"></i> arXiv</a> | 
              <a class="wrap" target="_blank" href="https://drive.google.com/open?id=1EGF5cbFQWRtHBwPbGfFWzjBJ3uajoqk4"><i class="fa fa-file-archive"></i> data</a>
            </td>
          </tr>

          <tr class="paper_entry" id="backhand">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two">
                  <img src="data/thumbnails/backhand_tn.png">
                </div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="https://dl.acm.org/doi/10.1145/3379337.3415897">
                <papertitle>Back-Hand-Pose: 3D Hand Pose Estimation for a Wrist-worn Camera via Dorsum Deformation Network</papertitle>
              </a>
              <br>
              <a class="wrap" target="_blank" href="http://jiaqi.web.illinois.edu">Erwin Wu</a>, 
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="https://hsyeo.me/">Hui-Shyong Yeo</a>, 
              <a class="wrap" target="_blank" href="https://aaronquigley.org/">Aaron Quigley</a>, 
              <a class="wrap" target="_blank" href="https://www.vogue.cs.titech.ac.jp/koike">Hideki Koike</a>, 
              <a class="wrap" target="_blank" href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a>
              <br>
              <span class="paper_conf">ACM Symposium on User Interface Software and Technology (UIST)</span>, <span class="paper_year">2020</span>
              <br>
              <a class="wrap" target="_blank" href="https://dl.acm.org/doi/10.1145/3379337.3415897"><i class="fa fa-file-pdf"></i> paper</a> | 
              <a class="wrap" target="_blank" href="https://drive.google.com/file/d/1rBxhrtjAPWvoDSoK0OrN20TfVJPIILEW/view?usp=sharing"><i class="fab fa-youtube"></i> video</a>
            </td>
          </tr>

          <tr class="paper_entry" id="monoeye">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two">
                  <img src="data/thumbnails/monoeye_tn.png">
                </div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="https://dl.acm.org/doi/10.1145/3379337.3415856">
                <papertitle>MonoEye: Multimodal Human Motion Capture System Using A Single Ultra-Wide Fisheye Camera</papertitle>
              </a>
              <br>
              <a class="wrap" target="_blank" href="https://hwangdonghyun.github.io/">Dong-Hyun Hwang</a>, 
              <a class="wrap" target="_blank" href="https://dblp.org/pid/247/3933.html">Kohei Aso</a>, 
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a>, 
              <a class="wrap" target="_blank" href="https://www.vogue.cs.titech.ac.jp/koike">Hideki Koike</a>
              <br>
              <span class="paper_conf">ACM Symposium on User Interface Software and Technology (UIST)</span>, <span class="paper_year">2020</span>
              <br>
              <a class="wrap" target="_blank" href="https://dl.acm.org/doi/10.1145/3379337.3415856"><i class="fa fa-file-pdf"></i> paper</a> | 
              <a class="wrap" target="_blank" href="https://youtu.be/jlxBagEsV1k"><i class="fab fa-youtube"></i> video</a>
            </td>
          </tr>

          <tr class="paper_entry" id="ego_pose">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two"><video width=100% height=100% muted autoplay loop playsinline>
                <source src="data/thumbnails/egopose_tn.mp4#t=0.001" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="https://www.ye-yuan.com/ego-pose">
                <papertitle>Ego-Pose Estimation and Forecasting as Real-Time PD Control</papertitle>
              </a>
              <br>
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a>
              <br>
              <span class="paper_conf">ICCV</span>, <span class="paper_year">2019</span>
              <br>
              <a class="wrap" target="_blank" href="https://www.ye-yuan.com/ego-pose"><i class="fa fa-globe"></i> project page</a> | 
              <a class="wrap" target="_blank" href="https://arxiv.org/abs/1906.03173"><i class="fa fa-file-pdf"></i> arXiv</a> | 
              <a class="wrap" target="_blank" href="https://youtu.be/968IIDZeWE0"><i class="fab fa-youtube"></i> video</a> | 
              <a class="wrap" target="_blank" href="https://github.com/Khrylx/EgoPose"><i class="fab fa-github"></i> code</a> | 
              <a class="wrap" target="_blank" href="https://drive.google.com/file/d/1vzxVHAtfvfIEDreqYvHulhtNwHcomotV/view?usp=sharing"><i class="fa fa-file-archive"></i> data</a>
            </td>
          </tr>

          <tr class="paper_entry" id="gailpose">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two">
                  <img src="data/thumbnails/gailpose_tn.png">
                </div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ye_Yuan_3D_Ego-Pose_Estimation_ECCV_2018_paper.pdf">
                <papertitle>3D Ego-Pose Estimation via Imitation Learning</papertitle>
              </a>
              <br>
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="http://www.cs.cmu.edu/~kkitani/">Kris Kitani</a>
              <br>
              <span class="paper_conf">ECCV</span>, <span class="paper_year">2018</span>
              <br>
              <a class="wrap" target="_blank" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ye_Yuan_3D_Ego-Pose_Estimation_ECCV_2018_paper.pdf"><i class="fa fa-file-pdf"></i> paper</a> | 
              <a class="wrap" target="_blank" href="https://youtu.be/a9GsGUpjxUs"><i class="fab fa-youtube"></i> video</a>
            </td>
          </tr>

          <tr class="paper_entry" id="tranformable">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two"><video width=100% height=100% muted autoplay loop playsinline>
                <source src="data/thumbnails/transformable_tn.mp4#t=0.001" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="http://crl.ethz.ch/papers/transformers.pdf">
                <papertitle>Computational Design of Transformables</papertitle>
              </a>
              <br>
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="http://www.cs.columbia.edu/~cxz/">Changxi Zheng</a>, 
              <a class="wrap" target="_blank" href="http://crl.ethz.ch/people/coros/">Stelian Coros</a>
              <br>
              <span class="paper_conf">ACM SIGGRAPH/Eurographics Symposium on Computer Animation (SCA)</span>, <span class="paper_year">2018</span>
              <br>
              <a class="wrap" target="_blank" href="http://crl.ethz.ch/papers/transformers.pdf"><i class="fa fa-file-pdf"></i> paper</a> | 
              <a class="wrap" target="_blank" href="https://youtu.be/cgejsrLPimg"><i class="fab fa-youtube"></i> video</a>
            </td>
          </tr>

          <tr class="paper_entry" id="modular">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two">
                  <img src="data/thumbnails/modular_tn.png">
                </div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ye_Yuan_3D_Ego-Pose_Estimation_ECCV_2018_paper.pdf">
                <papertitle>Computational Abstractions for Interactive Design of Robotic Devices</papertitle>
              </a>
              <br>
              <a class="wrap" target="_blank" href="https://www.cs.cmu.edu/~rutad/">Ruta Desai</a>, 
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="http://crl.ethz.ch/people/coros/">Stelian Coros</a>
              <br>
              <span class="paper_conf">ICRA</span>, <span class="paper_year">2017</span>
              <br>
              <a class="wrap" target="_blank" href="http://ri.cmu.edu/wp-content/uploads/2017/06/ICRAFinal.pdf"><i class="fa fa-file-pdf"></i> paper</a> | 
              <a class="wrap" target="_blank" href="https://youtu.be/PGpTsQtznw4"><i class="fab fa-youtube"></i> video</a>
            </td>
          </tr>

          <tr class="paper_entry" id="innercarve">
            <td class="paper_tb">
              <div class="tb_one">
                <div class="tb_two">
                  <img src="data/thumbnails/innercarve_tn.png">
                </div>
              </div>
            </td>
            <td class="paper_details">
              <a target="_blank" href="https://link.springer.com/article/10.1007/s11704-016-5465-y">
                <papertitle>Continuous Optimization of Interior Carving in 3D Fabrication</papertitle>
              </a>
              <br>
              Yue Xie, 
              <strong class="wrap">Ye Yuan</strong>, 
              <a class="wrap" target="_blank" href="https://xchen-cs.github.io/">Xiang Chen</a>, 
              <a class="wrap" target="_blank" href="http://www.cs.columbia.edu/~cxz/">Changxi Zheng</a>, 
              <a class="wrap" target="_blank" href="http://kunzhou.net/">Kun Zhou</a>
              <br>
              <span class="paper_conf">Frontiers of Computer Science</span>, <span class="paper_year">2017</span>
              <br>
              <a class="wrap" target="_blank" href="https://link.springer.com/article/10.1007/s11704-016-5465-y"><i class="fa fa-file-pdf"></i> paper</a>
            </td>
          </tr>

        </tr>
      </tr>
          

        </tbody></table>

        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px 20px 20px 20px;width:100%;vertical-align:middle;">
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-bottom:25px;"><tbody>
          <tr>
            <td class="service_name">
              Conference Reviewer
            </td>
            <td class="service_details">
              NeurIPS, ICML, ICLR, CVPR, ICCV, ECCV, AAAI, ICRA, SIGGRAPH, Eurographics
            </td>
          </tr>
          <tr>
            <td class="service_name">
              Journal Reviewer
            </td>
            <td class="service_details">
              JMLR, TMLR, TPAMI, TIP, RA-L
            </td>
          </tr>
        </tbody></table>
          
          
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Template adapted from <a href="https://jonbarron.info" target="_blank" style="font-size: small;">this awesome website</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
